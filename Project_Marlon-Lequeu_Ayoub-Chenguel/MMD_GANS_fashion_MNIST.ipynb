{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Hl271ZRyTo66"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 128\n",
        "latent_dim = 100\n",
        "num_epochs = 25\n",
        "lr_generator = 2e-4\n",
        "lr_critic = 2e-4\n",
        "n_critic = 5\n",
        "lambda_gp = 0\n",
        "log_interval = 100\n",
        "\n",
        "# Create directory for sample outputs\n",
        "os.makedirs(\"samples\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zETmNBBoT3-V"
      },
      "outputs": [],
      "source": [
        "# transform: scale images to [-1, 1] (to match generator's Tanh output)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# download dataset and create dataloader\n",
        "train_dataset = torchvision.datasets.FashionMNIST(\n",
        "    root=\"./data\", train=True, transform=transform, download=True\n",
        ")\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ht3yJyyUUJDo"
      },
      "outputs": [],
      "source": [
        "# Generator: maps a latent vector to a 28x28 single-channel image\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim=100):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        # Project latent vector into a 7x7 feature map (with 128 channels)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 7 * 7 * 128),\n",
        "            nn.BatchNorm1d(7 * 7 * 128),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        # Upsample with transposed convolutions to reach 28x28\n",
        "        self.deconv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # 7x7 -> 14x14\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1),    # 14x14 -> 28x28\n",
        "            nn.Tanh()  # output in [-1, 1] (assuming inputs are normalized accordingly)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        # Dense projection from latent space\n",
        "        x = self.fc(z)\n",
        "\n",
        "        # Reshape into (N, 128, 7, 7) before deconvolutions\n",
        "        x = x.view(-1, 128, 7, 7)\n",
        "\n",
        "        # Produce image through upsampling blocks\n",
        "        img = self.deconv(x)\n",
        "        return img\n",
        "\n",
        "\n",
        "# Critic: maps a 28x28 image to a feature vector (often used for scoring or further losses)\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        # Downsample the image into a compact 7x7 representation\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1),  # 28x28 -> 14x14\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),# 14x14 -> 7x7\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "\n",
        "        # Flatten and map to a 1024-dim feature vector\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128 * 7 * 7, 1024),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        # Convolutional feature extraction\n",
        "        x = self.conv(img)\n",
        "\n",
        "        # Final feature representation (returned as \"features\")\n",
        "        features = self.fc(x)\n",
        "        return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tOoRcu6WUKjM"
      },
      "outputs": [],
      "source": [
        "# useful fonctions, kernels and loss\n",
        "\n",
        "def compute_pairwise_distance(x, y):\n",
        "    n = x.size(0)\n",
        "    m = y.size(0)\n",
        "    x_norm = (x**2).sum(dim=1).view(n, 1)\n",
        "    y_norm = (y**2).sum(dim=1).view(1, m)\n",
        "    return x_norm + y_norm - 2 * torch.mm(x, y.t())\n",
        "\n",
        "def gaussian_kernel_matrix(x, y, sigmas=None):\n",
        "    if sigmas is None:\n",
        "        sigmas = [2, 5, 10, 20, 40, 80]\n",
        "    sigmas = torch.tensor(sigmas, device=x.device, dtype=x.dtype)\n",
        "    beta = 1. / (2. * (sigmas ** 2))  # Corrected: use sigma^2 in the denominator.\n",
        "    pairwise_dists = compute_pairwise_distance(x, y).unsqueeze(0)  # shape: (1, n, m)\n",
        "    beta = beta.view(-1, 1, 1)  # reshape for broadcasting\n",
        "    kernel_vals = torch.exp(- beta * pairwise_dists)\n",
        "    return kernel_vals.sum(dim=0)\n",
        "\n",
        "def linear_kernel_matrix(x, y):\n",
        "    return torch.mm(x, y.t())\n",
        "\n",
        "def rational_quadratic_kernel_matrix(x, y, alphas=None):\n",
        "    if alphas is None:\n",
        "        alphas = [0.2, 0.5, 1, 2, 5]\n",
        "    pairwise_dists = compute_pairwise_distance(x, y)  # squared distances\n",
        "    kernel_sum = 0\n",
        "    for alpha in alphas:\n",
        "        kernel_sum += (1 + pairwise_dists / (2 * alpha)) ** (-alpha)\n",
        "    return kernel_sum\n",
        "\n",
        "def mixed_rq_dot_kernel_matrix(x, y, alphas=None):\n",
        "    if alphas is None:\n",
        "        alphas = [0.2, 0.5, 1, 2, 5]\n",
        "    pairwise_dists = compute_pairwise_distance(x, y)\n",
        "    rq_kernel = 0\n",
        "    for alpha in alphas:\n",
        "        rq_kernel += (1 + pairwise_dists / (2 * alpha)) ** (-alpha)\n",
        "\n",
        "    linear_kernel = torch.mm(x, y.t())\n",
        "\n",
        "    return rq_kernel + linear_kernel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tiGjpCwpVah_"
      },
      "outputs": [],
      "source": [
        "def mmd_loss(real_features, fake_features, kernel=rational_quadratic_kernel_matrix, sigmas=None):\n",
        "    \"\"\"\n",
        "    Computes the (biased) MMD loss between real and fake feature batches using a kernel matrix.\n",
        "    \"\"\"\n",
        "\n",
        "    # Kernel matrix for real-real similarities\n",
        "    K_XX = kernel(real_features, real_features, sigmas)\n",
        "\n",
        "    # Kernel matrix for fake-fake similarities\n",
        "    K_YY = kernel(fake_features, fake_features, sigmas)\n",
        "\n",
        "    # Kernel matrix for real-fake similarities\n",
        "    K_XY = kernel(real_features, fake_features, sigmas)\n",
        "\n",
        "    # Batch sizes\n",
        "    m = real_features.size(0)\n",
        "    n = fake_features.size(0)\n",
        "\n",
        "    # Biased estimator: includes diagonal terms of K_XX and K_YY\n",
        "    loss = K_XX.sum()/(m*m) + K_YY.sum()/(n*n) - 2*K_XY.sum()/(m*n)\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_4rU3nVRVbt9"
      },
      "outputs": [],
      "source": [
        "def gradient_penalty(critic, real_imgs, fake_imgs, device, lambda_gp=10):\n",
        "    \"\"\"\n",
        "    WGAN-GP gradient penalty computed on random interpolations between real and fake samples.\n",
        "    \"\"\"\n",
        "\n",
        "    # per-sample mixing factor, broadcast over channels/spatial dims\n",
        "    alpha = torch.rand(real_imgs.size(0), 1, 1, 1, device=device)\n",
        "    # enable gradients w.r.t. inputs\n",
        "    interpolates = (alpha * real_imgs + (1 - alpha) * fake_imgs).requires_grad_(True)\n",
        "\n",
        "    scores = critic(interpolates)\n",
        "\n",
        "    # to aggregate per-sample outputs when calling autograd\n",
        "    grad_outputs = torch.ones_like(scores, device=device)\n",
        "    gradients = torch.autograd.grad(\n",
        "        outputs=scores,\n",
        "        inputs=interpolates,\n",
        "        grad_outputs=grad_outputs,\n",
        "        create_graph=True,   # keep graph so this penalty is differentiable\n",
        "        retain_graph=True,   # keep graph if you backprop through critic elsewhere\n",
        "        only_inputs=True\n",
        "    )[0]\n",
        "\n",
        "    # flatten per sample\n",
        "    gradients = gradients.view(gradients.size(0), -1)\n",
        "    # per-sample L2 norm\n",
        "    grad_norm = gradients.norm(2, dim=1)\n",
        "    # penalty term\n",
        "    gp = lambda_gp * ((grad_norm - 1) ** 2).mean()\n",
        "    return gp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R893_DdWULow"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# networks\n",
        "generator = Generator(latent_dim).to(device)\n",
        "critic    = Critic().to(device)\n",
        "\n",
        "# optimizers\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=lr_generator, betas=(0.5, 0.999))\n",
        "optimizer_C = optim.Adam(critic.parameters(), lr=lr_critic, betas=(0.5, 0.999))\n",
        "\n",
        "# fixed noise for evaluation\n",
        "fixed_noise = torch.randn(64, latent_dim, device=device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    fake = generator(fixed_noise).detach().cpu()\n",
        "grid = torchvision.utils.make_grid(fake, nrow=8, normalize=True)\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Initial Generated Samples\")\n",
        "plt.imshow(np.transpose(grid, (1,2,0)))\n",
        "plt.show()\n",
        "\n",
        "# training loop with sample display after each epoch\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    for batch_idx, (real_imgs, _) in enumerate(train_loader):\n",
        "        real_imgs = real_imgs.to(device)\n",
        "        batch_size_curr = real_imgs.size(0)\n",
        "\n",
        "        # critic training\n",
        "        for _ in range(n_critic):\n",
        "            noise = torch.randn(batch_size_curr, latent_dim, device=device)\n",
        "            fake_imgs = generator(noise).detach()  # detach to avoid generator update during critic update\n",
        "\n",
        "            real_features = critic(real_imgs)\n",
        "            fake_features = critic(fake_imgs)\n",
        "\n",
        "            mmd = mmd_loss(real_features, fake_features)\n",
        "            gp = gradient_penalty(critic, real_imgs, fake_imgs, device, lambda_gp)\n",
        "            critic_loss = -mmd + gp\n",
        "\n",
        "            optimizer_C.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            optimizer_C.step()\n",
        "\n",
        "        # generator\n",
        "        noise = torch.randn(batch_size_curr, latent_dim, device=device)\n",
        "        fake_imgs = generator(noise)\n",
        "        fake_features = critic(fake_imgs)\n",
        "        real_features = critic(real_imgs)\n",
        "        generator_loss = mmd_loss(real_features, fake_features)\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "        generator_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # training stats\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}] Batch {batch_idx}/{len(train_loader)} | \"\n",
        "                  f\"Critic Loss: {critic_loss.item():.4f} | Generator Loss: {generator_loss.item():.4f}\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        fake = generator(fixed_noise).detach().cpu()\n",
        "    grid = torchvision.utils.make_grid(fake, nrow=8, normalize=True)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8,8))\n",
        "    ax.imshow(np.transpose(grid, (1,2,0)))\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(f\"Epoch {epoch+1} Generated Samples\")\n",
        "    fig.savefig(f\"samples/epoch_{epoch+1:03d}.png\")\n",
        "    plt.show()\n",
        "    plt.close(fig)\n",
        "    clear_output(wait=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJByN6YqUOvm"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "sample_img = Image.open(f\"samples/epoch_{num_epochs:03d}.png\")\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(sample_img)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Generated Samples - Final Epoch\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmS-p4htqOP4"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "from IPython.display import FileLink\n",
        "\n",
        "shutil.make_archive('samples', 'zip', 'samples')\n",
        "FileLink(\"samples.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2QcSQNwqaew"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('samples.zip')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}